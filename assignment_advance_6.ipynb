{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d18ef4ef-95d7-4b03-9232-92f2d0bd2a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ques 1\n",
    "# Analysis of Variance (ANOVA) is a statistical method used to compare means among three or more groups. It is based on several assumptions, and violations of these assumptions can impact the validity of ANOVA results. The key assumptions for using ANOVA are:\n",
    "\n",
    "#1.> Independence: The observations within each group should be independent of each other. Violations could occur if observations within a group are correlated, such as in repeated measures designs where the same subjects are measured over time.\n",
    "\n",
    "#2.> Normality: The distribution of the residuals (the differences between observed values and predicted values) should be approximately normal for each group. Violations can occur if the residuals are significantly skewed or have heavy tails, leading to inaccurate p-values and confidence intervals.\n",
    "\n",
    "#3.> Homogeneity of Variance (Homoscedasticity): The variance of the residuals should be roughly constant across all groups. Violations could lead to unequal variability between groups, which may affect the ANOVA F-test and subsequent post hoc tests.\n",
    "\n",
    "#4.> Homogeneity of Regression Slopes (Interaction Assumption): This assumption is specific to two-way ANOVA (or higher-way ANOVA) when there are multiple independent variables (factors). It assumes that the relationship between the dependent variable and each independent variable is consistent across different levels of the other independent variables. Violations can result in misleading interpretations of main effects and interactions.\n",
    "\n",
    "# Examples of violations that could impact the validity of ANOVA results:\n",
    "\n",
    "#1.> Outliers: Outliers can lead to violations of the assumptions, especially normality and homogeneity of variance. They can skew the distribution of residuals and affect the accuracy of the F-test.\n",
    "\n",
    "#2.> Non-Normality: If the residuals are not normally distributed within groups, the ANOVA results may be invalid. This could occur when the data is heavily skewed or has extreme kurtosis.\n",
    "\n",
    "#3.> Heteroscedasticity: Unequal variances between groups can lead to inflated or deflated F-statistics, affecting the overall significance of the ANOVA. This violation can arise when the spread of residuals differs among groups.\n",
    "\n",
    "#4.> Violations of Independence: If observations are not independent within groups, it could lead to pseudo-replication, where the effective sample size is smaller than the actual sample size, potentially affecting the reliability of the results.\n",
    "\n",
    "#5.> Interactions: In designs with multiple factors, violating the assumption of homogeneity of regression slopes can distort the interpretation of main effects and interactions, leading to erroneous conclusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03e3bed5-d7a7-4e09-9680-875205f09d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ques 2\n",
    "# ans - \n",
    "# 1.> One- Way annova -- One factor with atleast 2 levels, these levels are independent.\n",
    "# example - Medication - (Factor) & 10 mg , 20 mg ,30mg are levels . \n",
    "\n",
    "#2.> Repeated Measure Annova :-- Onefactor with atleast 2 levels & levels are Dependent.\n",
    "# example-- Running(factor) & day1 , day2 ,day3 are levels.\n",
    "\n",
    "#3.> Factorial Annova : Two or more factor (each of which with 2 levels), levels can be either independent and dependent.\n",
    "# example-- running & Gender are two factor and Day1,Day2,Day3 are levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5694ae16-f100-4702-965f-dd9585926fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ques 3\n",
    "# ans - The partitioning of variance in Analysis of Variance (ANOVA) refers to the process of decomposing the total variability in a dataset into different components that can be attributed to different sources of variation. ANOVA aims to determine whether there are statistically significant differences between the means of multiple groups and to quantify the contributions of different factors to the overall variability in the data. This partitioning helps researchers understand the relative importance of various factors in explaining the variation observed in the dependent variable.\n",
    "\n",
    "# In ANOVA, the total variability in the data is broken down into three main components:\n",
    "\n",
    "# 1.> Between-Group Variance (SSB): This component represents the variability between the group means. It measures how much the group means differ from each other. A larger between-group variance suggests that there are significant differences between the group means.\n",
    "\n",
    "# 2.> Within-Group Variance (SSW): Also known as the \"error\" or \"residual\" variance, this component represents the variability within each group. It measures how much individual data points deviate from their respective group means. A smaller within-group variance indicates that the data points within each group are relatively similar.\n",
    "\n",
    "# 3.> Total Variance (SST): This is the overall variability in the entire dataset, regardless of group membership. It is the sum of the between-group variance and the within-group variance: SST = SSB + SSW.\n",
    "\n",
    "# The importance of understanding the partitioning of variance in ANOVA includes:\n",
    "\n",
    "#1.> Interpretation of Results: By decomposing the total variance into its components, researchers can better interpret the sources of variation that contribute to the observed differences between groups. This helps to provide a clearer understanding of the factors that are driving the significant results.\n",
    "\n",
    "#2.> Effect Size Calculation: The partitioning of variance is crucial for calculating effect sizes, such as eta-squared (η²) or partial eta-squared (η²p). These effect sizes quantify the proportion of total variance explained by the factors of interest, helping researchers assess the practical significance of their findings.\n",
    "\n",
    "#3.> Research Design and Hypotheses: Understanding the partitioning of variance can guide researchers in designing experiments and formulating hypotheses. For example, if the within-group variance is very high, it might suggest that other variables not included in the analysis are influencing the results.\n",
    "\n",
    "#4.> Model Assessment: The partitioning of variance provides a basis for assessing the adequacy of the ANOVA model. If the between-group variance is much larger than the within-group variance, it suggests that the groups are indeed different and that the ANOVA model is a good fit for the data.\n",
    "\n",
    "#5.> Comparative Analysis: ANOVA allows for the comparison of multiple groups simultaneously, which can provide insights into the relative performance or characteristics of different treatments, conditions, or group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48bdd86e-bdc3-4987-8697-93e9771fed2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sum of Squares (SST): 407.73333333333335\n",
      "Explained Sum of Squares (SSE): 252.13333333333333\n",
      "Residual Sum of Squares (SSR): 155.60000000000002\n"
     ]
    }
   ],
   "source": [
    "# Ques 4\n",
    "#ans --\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Sample data for each group\n",
    "group1 = np.array([23, 27, 29, 31, 32])\n",
    "group2 = np.array([18, 20, 22, 25, 28])\n",
    "group3 = np.array([15, 16, 18, 20, 23])\n",
    "\n",
    "# Combine data from all groups\n",
    "all_data = np.concatenate((group1, group2, group3))\n",
    "\n",
    "# Calculate overall mean\n",
    "overall_mean = np.mean(all_data)\n",
    "\n",
    "# Calculate Total Sum of Squares (SST)\n",
    "sst = np.sum((all_data - overall_mean)**2)\n",
    "\n",
    "# Calculate group means\n",
    "group1_mean = np.mean(group1)\n",
    "group2_mean = np.mean(group2)\n",
    "group3_mean = np.mean(group3)\n",
    "\n",
    "# Calculate Explained Sum of Squares (SSE)\n",
    "sse = np.sum((group1_mean - overall_mean)**2) * len(group1) + \\\n",
    "      np.sum((group2_mean - overall_mean)**2) * len(group2) + \\\n",
    "      np.sum((group3_mean - overall_mean)**2) * len(group3)\n",
    "\n",
    "# Calculate Residual Sum of Squares (SSR)\n",
    "ssr = sst - sse\n",
    "\n",
    "# Calculate degrees of freedom\n",
    "df_total = len(all_data) - 1\n",
    "df_groups = 3 - 1  # Number of groups minus 1\n",
    "df_residual = df_total - df_groups\n",
    "\n",
    "# Calculate mean square for groups (MS_groups) and for residuals (MS_residual)\n",
    "ms_groups = sse / df_groups\n",
    "ms_residual = ssr / df_residual\n",
    "\n",
    "# Calculate F-statistic\n",
    "f_statistic = ms_groups / ms_residual\n",
    "\n",
    "# Calculate p-value\n",
    "p_value = 1 - stats.f.cdf(f_statistic, df_groups, df_residual)\n",
    "\n",
    "print(\"Total Sum of Squares (SST):\", sst)\n",
    "print(\"Explained Sum of Squares (SSE):\", sse)\n",
    "print(\"Residual Sum of Squares (SSR):\", ssr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb1d8e39-e05a-479c-9c36-0d348f593a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main Effect A: [ 1.75       -1.58333333  3.41666667 -3.58333333]\n",
      "Main Effect B: [-2.41666667 -0.16666667  2.58333333]\n",
      "Interaction Effect: [[-0.25        0.5        -0.25      ]\n",
      " [ 0.08333333 -0.16666667  0.08333333]\n",
      " [ 0.08333333 -0.16666667  0.08333333]\n",
      " [ 0.08333333 -0.16666667  0.08333333]]\n",
      "F-statistic for Factor A: 2.320051413881747\n",
      "P-value for Factor A: nan\n",
      "F-statistic for Factor B: 1.9344473007712082\n",
      "P-value for Factor B: nan\n",
      "F-statistic for Interaction: 0.006426735218509008\n",
      "P-value for Interaction: nan\n"
     ]
    }
   ],
   "source": [
    "# Ques 5\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Sample data for each combination of factors A and B\n",
    "data = np.array([\n",
    "    [15, 18, 20],\n",
    "    [12, 14, 17],\n",
    "    [17, 19, 22],\n",
    "    [10, 12, 15]\n",
    "])\n",
    "\n",
    "# Calculate the overall mean\n",
    "overall_mean = np.mean(data)\n",
    "\n",
    "# Calculate the main effects for factors A and B\n",
    "main_effect_A = np.mean(data, axis=1) - overall_mean\n",
    "main_effect_B = np.mean(data, axis=0) - overall_mean\n",
    "\n",
    "# Calculate the interaction effect\n",
    "interaction_effect = np.zeros_like(data, dtype=float)\n",
    "for i in range(data.shape[0]):\n",
    "    for j in range(data.shape[1]):\n",
    "        interaction_effect[i, j] = data[i, j] - (main_effect_A[i] + main_effect_B[j] + overall_mean)\n",
    "\n",
    "# Calculate the Total Sum of Squares (SST)\n",
    "sst = np.sum((data - overall_mean)**2)\n",
    "\n",
    "# Calculate the Main Effects Sum of Squares (SSE_main) for factors A and B\n",
    "sse_main_A = np.sum(main_effect_A**2) * data.shape[1]\n",
    "sse_main_B = np.sum(main_effect_B**2) * data.shape[0]\n",
    "\n",
    "# Calculate the Interaction Effect Sum of Squares (SSE_interaction)\n",
    "sse_interaction = np.sum(interaction_effect**2)\n",
    "\n",
    "# Calculate the Residual Sum of Squares (SSR)\n",
    "ssr = sst - sse_main_A - sse_main_B - sse_interaction\n",
    "\n",
    "# Calculate degrees of freedom\n",
    "df_total = data.size - 1\n",
    "df_factor_A = data.shape[0] - 1\n",
    "df_factor_B = data.shape[1] - 1\n",
    "df_interaction = df_factor_A * df_factor_B\n",
    "df_residual = df_total - df_factor_A - df_factor_B - df_interaction\n",
    "\n",
    "# Calculate mean squares for factors A, B, and interaction (MS_factor_A, MS_factor_B, MS_interaction)\n",
    "ms_factor_A = sse_main_A / df_factor_A\n",
    "ms_factor_B = sse_main_B / df_factor_B\n",
    "ms_interaction = sse_interaction / df_interaction\n",
    "\n",
    "# Calculate F-statistics for factors A, B, and interaction\n",
    "f_statistic_factor_A = ms_factor_A / ms_residual\n",
    "f_statistic_factor_B = ms_factor_B / ms_residual\n",
    "f_statistic_interaction = ms_interaction / ms_residual\n",
    "\n",
    "# Calculate p-values for factors A, B, and interaction\n",
    "p_value_factor_A = 1 - stats.f.cdf(f_statistic_factor_A, df_factor_A, df_residual)\n",
    "p_value_factor_B = 1 - stats.f.cdf(f_statistic_factor_B, df_factor_B, df_residual)\n",
    "p_value_interaction = 1 - stats.f.cdf(f_statistic_interaction, df_interaction, df_residual)\n",
    "\n",
    "print(\"Main Effect A:\", main_effect_A)\n",
    "print(\"Main Effect B:\", main_effect_B)\n",
    "print(\"Interaction Effect:\", interaction_effect)\n",
    "print(\"F-statistic for Factor A:\", f_statistic_factor_A)\n",
    "print(\"P-value for Factor A:\", p_value_factor_A)\n",
    "print(\"F-statistic for Factor B:\", f_statistic_factor_B)\n",
    "print(\"P-value for Factor B:\", p_value_factor_B)\n",
    "print(\"F-statistic for Interaction:\", f_statistic_interaction)\n",
    "print(\"P-value for Interaction:\", p_value_interaction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d84fab8-2ecb-45ad-9ebe-9d861067f6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ques 6\n",
    "# In a one-way ANOVA, the F-statistic is used to test the null hypothesis that all group means are equal against the alternative hypothesis that at least one group mean is different from the others. The p-value associated with the F-statistic indicates the probability of obtaining a result as extreme as the observed result under the assumption that the null hypothesis is true.\n",
    "\n",
    "# Given an F-statistic of 5.23 and a p-value of 0.02, here's what you can conclude and how you would interpret the results:\n",
    "\n",
    "#1.> Conclusion about Group Differences:\n",
    "\n",
    "# The F-statistic of 5.23 indicates that there is some variability in the means of the groups.\n",
    "# The p-value of 0.02 is less than the commonly used significance level of 0.05 (or 5%).\n",
    "# Since the p-value is less than 0.05, you would reject the null hypothesis at the 0.05 significance level.\n",
    "# This means that you have evidence to conclude that there are significant differences between at least some of the groups.\n",
    "#2.> Interpretation:\n",
    "\n",
    "# The F-statistic of 5.23 suggests that the variability between the group means is 5.23 times larger than the variability within the groups.\n",
    "# The p-value of 0.02 suggests that the probability of observing such a large F-statistic under the assumption of no group differences is only 0.02 (or 2%).\n",
    "# In practical terms, these results indicate that the groups are not all the same and that there are statistically significant differences between the groups' means.\n",
    "# However, the ANOVA does not tell you which specific groups are different from each other; it only indicates the presence of differences.\n",
    "#3.> Further Analysis:\n",
    "\n",
    "#To determine which specific groups are different from each other, you might consider conducting post hoc tests (e.g., Tukey's HSD, Bonferroni, or others).\n",
    "# These tests help identify the specific pairs of groups that have significantly different means.\n",
    "#It's important to note that while the p-value is a valuable indicator of the strength of evidence against the null hypothesis, it doesn't provide information about the magnitude of the effect or the practical significance of the differences. Effect size measures (such as eta-squared) can help provide insights into the size of the differences between groups.\n",
    "\n",
    "# In summary, with an F-statistic of 5.23 and a p-value of 0.02, you can conclude that there are significant differences between the groups' means. However, additional post hoc tests or further analysis would be necessary to determine which specific groups are different from each other.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "290ee3d6-1818-4f58-8ce4-c9602015b502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ques 7 \n",
    "#ans-- Handling missing data in a repeated measures ANOVA (Analysis of Variance) is an important consideration to ensure the validity and accuracy of your results. There are several methods to handle missing data, each with its own potential consequences. Let's explore the options and their implications:\n",
    "\n",
    "#1.> Listwise Deletion (Complete Case Analysis):\n",
    "\n",
    "# In this approach, cases with missing data on any variable are excluded from the analysis.\n",
    "# Pros: It is straightforward and retains cases with complete data.\n",
    "# Cons: It can lead to biased results if missing data are not missing completely at random (MCAR). It can also reduce statistical power and result in an unrepresentative sample.\n",
    "\n",
    "#2.> Pairwise Deletion:\n",
    "\n",
    "# Cases with missing data for a specific comparison are excluded only from that specific comparison, allowing all other data to be used.\n",
    "# Pros: Maximizes use of available data and can provide more information than listwise deletion.\n",
    "# Cons: Can lead to different degrees of freedom for different comparisons, which may complicate interpretation. Similar to listwise deletion, it may introduce bias if missingness is not MCAR.\n",
    "\n",
    "#3.> Mean Imputation:\n",
    "\n",
    "# Missing values are replaced with the mean of the available data for the respective variable.\n",
    "# Pros: Simple to implement and can maintain the original sample size.\n",
    "# Cons: Can lead to underestimation of variability and correlations between variables, as well as distortion of relationships between variables.\n",
    "\n",
    "#4.> Last Observation Carried Forward (LOCF) or Next Observation Carried Backward (NOCB):\n",
    "\n",
    "# Missing data are replaced with the last available observation (LOCF) or the next available observation (NOCB).\n",
    "# Pros: Simple to implement and maintains the time sequence of data.\n",
    "# Cons: Can introduce bias if the missing data patterns are related to the values themselves, leading to inaccurate estimates.\n",
    "\n",
    "#5.> Multiple Imputation:\n",
    "\n",
    "# Missing data are imputed multiple times, creating several complete datasets. Analysis is conducted on each dataset, and results are pooled to obtain final estimates.\n",
    "# Pros: Provides more accurate estimates by accounting for uncertainty related to missing data. Handles missingness that is not MCAR.\n",
    "# Cons: Can be computationally intensive and may require assumptions about the missing data mechanism.\n",
    "\n",
    "#6.> Model-Based Imputation:\n",
    "\n",
    "# Imputes missing values based on a model fitted to the observed data.\n",
    "# Pros: Can produce accurate imputations when the model assumptions hold.\n",
    "# Cons: Model misspecification can lead to biased results.\n",
    "\n",
    "#7.> Interpolation or Extrapolation:\n",
    "\n",
    "# Missing data are estimated using interpolation (for missing data within the observed range) or extrapolation (for missing data outside the observed range).\n",
    "# Pros: Can be appropriate when the data follow a predictable pattern.\n",
    "# Cons: Extrapolation can be risky and may lead to unreliable estimates.\n",
    "# The consequences of using different methods to handle missing data include potential bias, inflated or deflated standard errors, incorrect p-values, and distorted relationships between variables. The choice of method should be guided by the nature of the missing data, the underlying assumptions, and the goals of the analysis. It's important to transparently report the chosen method and any potential limitations associated with it.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4f59bc2-c8f1-41d7-9cf8-4f8203adc6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ques 8 \n",
    "#ans-- Post-hoc tests are used to make pairwise comparisons between groups after obtaining a significant result in an Analysis of Variance (ANOVA) to determine which specific groups differ from each other. When you have three or more groups and ANOVA indicates a significant overall effect, post-hoc tests help identify which group differences are responsible for that effect. Some common post-hoc tests include:\n",
    "\n",
    "#1.> Tukey's Honestly Significant Difference (HSD):\n",
    "\n",
    "# Use when you want to control the familywise error rate (the probability of making at least one Type I error across all comparisons) at a specific level (often 0.05).\n",
    "# Suitable for balanced designs (equal group sizes) and when you have a moderate to large number of groups.\n",
    "# Example: A researcher conducts an experiment comparing the performance of four different teaching methods (A, B, C, D) on students' test scores. ANOVA shows a significant difference between at least some of the groups. Tukey's HSD is used to determine which specific pairs of teaching methods are significantly different from each other.\n",
    "\n",
    "#2.> Bonferroni Correction:\n",
    "\n",
    "# Use when you need a more conservative approach to control the familywise error rate across multiple comparisons.\n",
    "# Divides the desired significance level (e.g., 0.05) by the number of comparisons, resulting in a stricter threshold for individual p-values.\n",
    "# Example: A clinical trial assesses the effectiveness of three different drug treatments (X, Y, Z) on a particular medical condition. ANOVA suggests a significant difference between treatments. To account for multiple comparisons, Bonferroni correction is applied to compare each treatment to the others while maintaining an overall significance level.\n",
    "\n",
    "#3.> Dunn's Test:\n",
    "\n",
    "# Use when you have a small sample size or unequal group sizes and are concerned about the assumptions of other post-hoc tests.\n",
    "# It's a non-parametric alternative to Tukey's HSD or Bonferroni correction.\n",
    "# Example: A study examines the effect of different exercise regimes (High Intensity, Moderate Intensity, Low Intensity) on weight loss. ANOVA indicates a significant difference. Since the assumptions of parametric tests might not hold well, Dunn's test is employed to make pairwise comparisons.\n",
    "\n",
    "#5.> Scheffe's Method:\n",
    "\n",
    "# Use when you want a more conservative approach to control the familywise error rate, especially when sample sizes are unequal and the variances are not homogeneous.\n",
    "# Offers broader protection against Type I errors but is less powerful than other methods.\n",
    "# Example: An experiment investigates the impact of three different marketing strategies (A, B, C) on sales across different regions. ANOVA reveals a significant effect. Scheffe's method is chosen to compare the strategies due to its robustness.\n",
    "\n",
    "#5.>Fisher's LSD (Least Significant Difference):\n",
    "\n",
    "# Use when you want a simple and exploratory approach to pairwise comparisons.\n",
    "# It's less stringent than some other post-hoc tests and might be used when you're exploring potential group differences before conducting more stringent tests.\n",
    "# Example: An analysis examines the effect of various doses of a drug (5 mg, 10 mg, 20 mg) on blood pressure. ANOVA shows a significant difference. Fisher's LSD is employed for exploratory pairwise comparisons before considering other post-hoc tests.\n",
    "# Remember, the choice of post-hoc test should be based on the specific characteristics of your data, the research question, and the assumptions of each test. It's important to adjust for multiple comparisons to control the overall Type I error rate and to report the chosen post-hoc test and its results transparently in your analysis.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b7075e7-02aa-42ef-a33d-5217d0c701ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-statistic: 153.0731705736759\n",
      "p-value: 1.1595673877935681e-36\n",
      "There is a significant difference between the mean weight loss of the three diets.\n"
     ]
    }
   ],
   "source": [
    "# Ques 9 \n",
    "# ans -- Certainly! Here's an example of how you can conduct a one-way ANOVA in Python using the scipy.stats library to analyze the mean weight loss of three diets (A, B, and C) based on the data collected from 50 participants\n",
    "\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Simulated weight loss data for each diet\n",
    "diet_A = np.array([3.2, 4.5, 2.7, 5.1, 3.9, 4.3, 3.8, 4.2, 2.5, 3.6,\n",
    "                   4.1, 2.9, 3.8, 4.0, 3.7, 4.5, 3.4, 4.1, 3.3, 3.0,\n",
    "                   4.2, 3.6, 3.5, 4.8, 4.4, 3.2, 3.9, 4.1, 3.7, 4.0,\n",
    "                   3.3, 4.2, 2.8, 4.5, 3.6, 3.9, 4.1, 3.7, 2.9, 4.4,\n",
    "                   3.1, 4.3, 3.6, 3.8, 4.0, 2.7, 3.5, 4.2, 3.0, 4.4])\n",
    "\n",
    "diet_B = np.array([2.8, 3.9, 2.2, 3.1, 3.5, 3.8, 3.4, 3.7, 2.5, 3.0,\n",
    "                   3.3, 2.9, 3.6, 3.1, 3.2, 3.8, 2.7, 3.4, 2.9, 2.8,\n",
    "                   3.5, 3.2, 3.0, 3.6, 3.1, 2.7, 3.4, 3.5, 3.0, 3.6,\n",
    "                   2.8, 3.9, 2.7, 3.4, 3.1, 3.6, 3.3, 2.6, 3.8, 3.4,\n",
    "                   2.9, 3.7, 3.1, 3.3, 3.5, 2.5, 3.0, 3.8, 2.8, 3.5])\n",
    "\n",
    "diet_C = np.array([1.9, 2.6, 2.0, 1.8, 2.3, 2.7, 2.5, 2.1, 2.8, 1.7,\n",
    "                   2.4, 1.6, 2.3, 2.2, 1.9, 2.6, 2.0, 2.1, 1.8, 2.0,\n",
    "                   2.4, 2.2, 2.3, 2.5, 2.1, 1.7, 2.6, 2.3, 2.0, 2.5,\n",
    "                   1.8, 2.4, 1.5, 2.7, 2.2, 2.4, 2.6, 1.6, 2.5, 2.0,\n",
    "                   2.3, 2.1, 2.2, 2.4, 2.6, 1.7, 1.9, 2.3, 1.8, 2.2])\n",
    "\n",
    "# Perform one-way ANOVA\n",
    "f_statistic, p_value = stats.f_oneway(diet_A, diet_B, diet_C)\n",
    "\n",
    "# Print results\n",
    "print(\"F-statistic:\", f_statistic)\n",
    "print(\"p-value:\", p_value)\n",
    "\n",
    "# Interpret the results\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    print(\"There is a significant difference between the mean weight loss of the three diets.\")\n",
    "else:\n",
    "    print(\"There is no significant difference between the mean weight loss of the three diets.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0c97518-f38a-40e3-935c-03619e134af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this example, we first simulate weight loss data for each diet using NumPy arrays. We then use the stats.f_oneway function to perform the one-way ANOVA. The F-statistic and p-value are printed, followed by an interpretation of the results based on the chosen significance level (alpha = 0.05).\n",
    "\n",
    "# Remember that the data provided here is simulated for illustrative purposes. In a real-world scenario, you would replace the simulated data with your actual data collected from the study participants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54754256-2f89-4bd0-839a-cdc6d29f3a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       df      sum_sq   mean_sq         F    PR(>F)\n",
      "Software              2.0    2.323530  1.161765  0.327679  0.722017\n",
      "Experience            1.0    0.026471  0.026471  0.007466  0.931463\n",
      "Software:Experience   2.0    0.993526  0.496763  0.140114  0.869575\n",
      "Residual             54.0  191.453381  3.545433       NaN       NaN\n",
      "\n",
      "Main effect of Software:\n",
      "There is no significant main effect of Software.\n",
      "\n",
      "Main effect of Experience:\n",
      "There is no significant main effect of Experience.\n",
      "\n",
      "Interaction effect between Software and Experience:\n",
      "There is no significant interaction effect between Software and Experience.\n"
     ]
    }
   ],
   "source": [
    "# Ques 10 \n",
    "# ans --\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.anova import anova_lm\n",
    "\n",
    "# Simulated data\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create data frame\n",
    "data = pd.DataFrame({\n",
    "    'Software': np.repeat(['Program A', 'Program B', 'Program C'], 20),\n",
    "    'Experience': np.tile(['Novice', 'Experienced'], 30),\n",
    "    'Time': np.random.normal(10, 2, 60)  # Simulated task completion times\n",
    "})\n",
    "\n",
    "# Perform two-way ANOVA\n",
    "formula = 'Time ~ Software + Experience + Software:Experience'\n",
    "model = ols(formula, data).fit()\n",
    "anova_results = anova_lm(model)\n",
    "\n",
    "# Print ANOVA results\n",
    "print(anova_results)\n",
    "\n",
    "# Interpret the results\n",
    "alpha = 0.05\n",
    "\n",
    "# Main effect of Software\n",
    "print(\"\\nMain effect of Software:\")\n",
    "software_p_value = anova_results.loc['Software', 'PR(>F)']\n",
    "if software_p_value < alpha:\n",
    "    print(\"There is a significant main effect of Software.\")\n",
    "else:\n",
    "    print(\"There is no significant main effect of Software.\")\n",
    "\n",
    "# Main effect of Experience\n",
    "print(\"\\nMain effect of Experience:\")\n",
    "experience_p_value = anova_results.loc['Experience', 'PR(>F)']\n",
    "if experience_p_value < alpha:\n",
    "    print(\"There is a significant main effect of Experience.\")\n",
    "else:\n",
    "    print(\"There is no significant main effect of Experience.\")\n",
    "\n",
    "# Interaction effect between Software and Experience\n",
    "print(\"\\nInteraction effect between Software and Experience:\")\n",
    "interaction_p_value = anova_results.loc['Software:Experience', 'PR(>F)']\n",
    "if interaction_p_value < alpha:\n",
    "    print(\"There is a significant interaction effect between Software and Experience.\")\n",
    "else:\n",
    "    print(\"There is no significant interaction effect between Software and Experience.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d3e684-b615-46e2-8a94-7bfe2980b267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ques 11\n",
    "# ans-- "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
